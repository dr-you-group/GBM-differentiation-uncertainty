{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5b4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from itertools import cycle\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58cd55",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb183e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../'\n",
    "\n",
    "with open(os.path.join(data_dir, 'x_train.pickle'), 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "with open(os.path.join(data_dir, 'x_valid.pickle'), 'rb') as f:\n",
    "    x_valid = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_dir, 'x_test.pickle'), 'rb') as f:\n",
    "    x_test = pickle.load(f)\n",
    "with open(os.path.join(data_dir, 'x_ext.pickle'), 'rb') as f:\n",
    "    x_ext = pickle.load(f)\n",
    "   \n",
    "with open(os.path.join(data_dir, 'y_train.pickle'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open(os.path.join(data_dir, 'y_valid.pickle'), 'rb') as f:\n",
    "    y_valid = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_dir, 'y_test.pickle'), 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open(os.path.join(data_dir, 'y_ext.pickle'), 'rb') as f:\n",
    "    y_ext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_valid = keras.utils.to_categorical(y_valid)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "y_ext = keras.utils.to_categorical(y_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f504842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape, x_ext.shape, y_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2058883",
   "metadata": {},
   "source": [
    "# 2. Data Visualization with Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690333a6",
   "metadata": {},
   "source": [
    "## 2.1. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41299d8",
   "metadata": {
    "id": "c41299d8"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=90,\n",
    "                                   zoom_range=[0.8, 1.0], \n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True, \n",
    "                                   fill_mode='constant', cval=0)\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab747-35f0-4291-989e-0688efadd9a6",
   "metadata": {},
   "source": [
    "## 2.1. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train visualization\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "axs = []\n",
    "\n",
    "for x_data in train_datagen.flow(x_train, seed=42):\n",
    "    for idx, img in enumerate(x_data):\n",
    "        axs.append(fig.add_subplot(4,8,idx+1))\n",
    "        plt.imshow(img)\n",
    "        axs[idx].axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d375a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_valid visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "axs = []\n",
    "\n",
    "for x_data in test_datagen.flow(x_valid, shuffle=False, seed=42):\n",
    "    for idx, img in enumerate(x_data):\n",
    "        axs.append(fig.add_subplot(5,10,idx+1))\n",
    "        plt.imshow(img)\n",
    "        axs[idx].axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73640d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "axs = []\n",
    "\n",
    "for x_data in test_datagen.flow(x_test, shuffle=False, seed=42):\n",
    "    for idx, img in enumerate(x_data):\n",
    "        axs.append(fig.add_subplot(5,10,idx+1))\n",
    "        plt.imshow(img)\n",
    "        axs[idx].axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b042a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ext visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "axs = []\n",
    "\n",
    "for x_data in test_datagen.flow(x_ext, shuffle=False, seed=42):\n",
    "    for idx, img in enumerate(x_data):\n",
    "        axs.append(fig.add_subplot(5,10,idx+1))\n",
    "        plt.imshow(img)\n",
    "        axs[idx].axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6c394",
   "metadata": {
    "id": "05a6c394"
   },
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9a790",
   "metadata": {
    "id": "eaa9a790"
   },
   "source": [
    "## 3.1. Deep Ensembles Train  \n",
    "log synced with wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = os.path.join(os.getcwd(), 'BestModelSaved')\n",
    "model_num = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1a41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "#train networks\n",
    "nets = 5\n",
    "model = [0] *nets\n",
    "single_proba=[]\n",
    "ext_single_proba=[]\n",
    "\n",
    "for j in range(nets):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model_name= model_num + str(j+1)\n",
    "    model_path = os.path.join(save_model, model_name + '_bestmodel.h5')\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                                    monitor='val_accuracy',\n",
    "                                                    verbose=1,\n",
    "                                                    save_weights_only=False,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='auto',\n",
    "                                                    save_freq='epoch')\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "    \n",
    "    \n",
    "    run = wandb.init(project='Brain_GBMmeta', group=model_num, \n",
    "                     config={\n",
    "                         'batch_size':32, \n",
    "                         'epochs':50,\n",
    "                         'loss_function':'categorical_crossentropy', \n",
    "                         'architecture':'DenseNet121 pretrained', \n",
    "                         'dataset':'DataProcessed/final_twoT2'\n",
    "                     }, \n",
    "                    name=model_name)\n",
    "    config = wandb.config\n",
    "    \n",
    "    \n",
    "    densenet121=DenseNet121()\n",
    "    \n",
    "    model[j] = densenet121.layers[-2].output\n",
    "    model[j] = tf.keras.layers.Dense(2, activation='softmax')(model[j])\n",
    "    model[j] = tf.keras.models.Model(inputs=densenet121.input, outputs=model[j])\n",
    "    \n",
    "    model[j].compile(loss=config.loss_function, \n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    _ = model[j].fit(train_datagen.flow(x_train, y_train, batch_size=config.batch_size, seed=42),\n",
    "                     validation_data=test_datagen.flow(x_valid, y_valid, shuffle=False, batch_size=config.batch_size, seed=42),\n",
    "                     epochs=config.epochs,\n",
    "                     steps_per_epoch=x_train.shape[0]//32, \n",
    "                     callbacks=[annealer, checkpoint, early_stopping, WandbCallback(monitor='val_accuracy')],\n",
    "                     verbose=1)\n",
    "    \n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f} \\n\".format(\n",
    "        j+1,config.epochs,max(_.history['accuracy']),max(_.history['val_accuracy']) ))\n",
    "    \n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc022c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Load saved model\n",
    "nets = 5\n",
    "de_model = [0]*5\n",
    "de_model[0] = keras.models.load_model(\"0_bestmodel.h5\")\n",
    "de_model[1] = keras.models.load_model(\"1_bestmodel.h5\")\n",
    "de_model[2] = keras.models.load_model(\"2_bestmodel.h5\")\n",
    "de_model[3] = keras.models.load_model(\"3_bestmodel.h5\")\n",
    "de_model[4] = keras.models.load_model(\"4_bestmodel.h5\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-combination",
   "metadata": {},
   "source": [
    "## 3.2. Sinlge Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = os.path.join(os.getcwd(), 'BestModelSaved')\n",
    "model_num = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-curve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_name= model_num\n",
    "model_path = os.path.join(save_model, model_name + '_bestmodel.h5')\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                                monitor='val_accuracy',\n",
    "                                                verbose=1,\n",
    "                                                save_weights_only=False,\n",
    "                                                save_best_only=True,\n",
    "                                                mode='auto',\n",
    "                                                save_freq='epoch')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "\n",
    "\n",
    "run = wandb.init(project='Brain_GBMmeta', group=model_num, \n",
    "                 config={\n",
    "                     'batch_size':32, \n",
    "                     'epochs':50,\n",
    "                     'loss_function':'categorical_crossentropy', \n",
    "                     'architecture':'DenseNet121 pretrained', \n",
    "                     'dataset':'DataProcessed/final_twoT2'\n",
    "                 }, \n",
    "                name=model_name)\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "densenet121=DenseNet121()\n",
    "\n",
    "model = densenet121.layers[-2].output\n",
    "model = tf.keras.layers.Dense(2, activation='softmax')(model)\n",
    "model = tf.keras.models.Model(inputs=densenet121.input, outputs=model)\n",
    "\n",
    "model.compile(loss=config.loss_function, \n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "_ = model.fit(train_datagen.flow(x_train, y_train, batch_size=config.batch_size, seed=42),\n",
    "                 validation_data=test_datagen.flow(x_valid, y_valid, shuffle=False, batch_size=config.batch_size, seed=42),\n",
    "                 epochs=config.epochs,\n",
    "                 steps_per_epoch=x_train.shape[0]//32, \n",
    "                 callbacks=[annealer, checkpoint, early_stopping, WandbCallback(monitor='val_accuracy')],\n",
    "                 verbose=1)\n",
    "\n",
    "print(\"CNN Single: Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f} \\n\".format(\n",
    "    config.epochs,max(_.history['accuracy']),max(_.history['val_accuracy']) ))\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3173843",
   "metadata": {
    "id": "c3173843"
   },
   "source": [
    "# 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0337df3",
   "metadata": {
    "id": "f0337df3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, auc, roc_auc_score, recall_score, f1_score, balanced_accuracy_score, classification_report, roc_curve, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43398c",
   "metadata": {
    "id": "0d43398c"
   },
   "outputs": [],
   "source": [
    "def matrix(y_true, y_pred, y_proba):\n",
    "    conf_mat = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    TP = conf_mat[1][1]\n",
    "    TN = conf_mat[0][0]\n",
    "    FN = conf_mat[1][0]\n",
    "    FP = conf_mat[0][1]\n",
    "\n",
    "    acc = round((TP+TN)/(TP+TN+FP+FN), 4)\n",
    "    sens = round(TP/(TP+FN), 4)\n",
    "    spec = round(TN/(TN+FP), 4)\n",
    "    auroc = round(roc_auc_score(y_true, y_proba), 4)\n",
    "    precision = round(precision_score(y_true.argmax(axis=1), y_pred.argmax(axis=1)), 4)\n",
    "    recall = round(recall_score(y_true.argmax(axis=1), y_pred.argmax(axis=1)), 4)\n",
    "    f1score = round(f1_score(y_true.argmax(axis=1), y_pred.argmax(axis=1)), 4)\n",
    "    w_acc = round(balanced_accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1)), 4)\n",
    "\n",
    "    print(conf_mat, \"\\n\")\n",
    "    print(\"Acc\", acc)\n",
    "    print(\"Sensitivity\", sens)\n",
    "    print(\"Specificity\", spec)\n",
    "    print(\"AUROC\", auroc)\n",
    "    print('Precision', precision)\n",
    "    print(\"Recall\", recall)\n",
    "    print(\"F1\",  f1score)\n",
    "    print(\"Weighted accuracy\", w_acc, \"\\n\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # ROC & AUC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    colors = cycle(['blue', 'red'])\n",
    "    for i, color in zip(range(y_true.shape[1]), colors):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of {0} (area = {1:0.2f})' ''.format(label[str(i)], roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for multi-class data')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {'0': 'GBM', \n",
    "        '1':'meta'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--uNv_7Y3X4q",
   "metadata": {
    "id": "--uNv_7Y3X4q"
   },
   "source": [
    "## 4.1. Internal Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-draft",
   "metadata": {},
   "source": [
    "### 4.1.1. Deep Ensembles Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be690f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "de_sing_proba= []\n",
    "for m in de_model:\n",
    "    proba = m.predict(test_datagen.flow(x_test, shuffle=False, seed=42))\n",
    "    de_sing_proba.append(proba)\n",
    "\n",
    "# Ensemble\n",
    "results = np.zeros( (y_test.shape[0],2) )\n",
    "for s in de_sing_proba:\n",
    "    results = results + s\n",
    "\n",
    "de_proba = results/5\n",
    "de_pred = (de_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esemble Result\n",
    "matrix(y_test, de_pred, de_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-experiment",
   "metadata": {},
   "source": [
    "### 4.1.2. Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "sing_proba = sing_model.predict(test_datagen.flow(x_test, shuffle=False, seed=42))\n",
    "sing_pred = (sing_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, sing_pred.shape, sing_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_test, sing_pred, sing_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_test, sing_pred, sing_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e61e43",
   "metadata": {
    "id": "64e61e43"
   },
   "source": [
    "## 4.2. External Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-migration",
   "metadata": {},
   "source": [
    "### 4.2.1. Deep Ensembles Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8116c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensemble\n",
    "ext_de_sing_proba= []\n",
    "for m in de_model:\n",
    "    proba = m.predict(test_datagen.flow(x_ext, shuffle=False, seed=42))\n",
    "    ext_de_sing_proba.append(proba)\n",
    "    \n",
    "# Ensemble\n",
    "ext_results = np.zeros( (y_ext.shape[0],2) )\n",
    "for s in ext_de_sing_proba:\n",
    "    ext_results = ext_results + s\n",
    "\n",
    "ext_de_proba = ext_results/5\n",
    "ext_de_pred = (ext_de_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_ext, ext_de_pred, ext_de_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-defendant",
   "metadata": {},
   "source": [
    "### 4.2.2. Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_Single\n",
    "ext_sing_proba = sing_model.predict(test_datagen.flow(x_ext, shuffle=False, seed=42))\n",
    "ext_sing_pred = (ext_sing_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_ext, ext_sing_pred, ext_sing_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d5971-9826-4c7c-878d-24fc96c5b789",
   "metadata": {},
   "source": [
    "## 4.3. Uncertainty (Entropy) calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb51fb0-09b0-48f7-9bea-7807b9f763d1",
   "metadata": {},
   "source": [
    "### 4.3.1. Internal test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d353a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensembles\n",
    "int_de_entropy_list = []\n",
    "for i in range(len(de_proba)):\n",
    "    pq = de_proba[i] * y_test[i] #정답에 대한 예측 확률 사용하는거 맞는지 확인\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    int_de_entropy_list.append(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single \n",
    "int_sing_entropy_list = []\n",
    "for i in range(len(sing_proba)):\n",
    "    pq = sing_proba[i] * y_test[i] #정답에 대한 예측 확률 사용하는거 맞는지 확인\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    int_sing_entropy_list.append(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6905ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution\n",
    "sns.kdeplot(int_de_entropy_list, label='Deep Ensembles', color='r')\n",
    "sns.kdeplot(int_sing_entropy_list, label='Single', color='g')\n",
    "plt.xlabel('Internal Validation Entropy Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8fc87-5b9e-433e-b187-8a7ff6ad75f9",
   "metadata": {},
   "source": [
    "### 4.3.2. External Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c943c6",
   "metadata": {
    "id": "36c943c6"
   },
   "outputs": [],
   "source": [
    "# Deep Ensembles\n",
    "ext_de_entropy_list = []\n",
    "for i in range(len(ext_de_proba)):\n",
    "    pq = ext_de_proba[i] * y_ext[i] #정답에 대한 예측 확률 사용하는거 맞는지 확인\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    ext_de_entropy_list.append(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "ext_sing_entropy_list = []\n",
    "for i in range(len(ext_sing_proba)):\n",
    "    pq = ext_sing_proba[i] * y_ext[i]\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    ext_sing_entropy_list.append(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f22f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution\n",
    "sns.kdeplot(ext_de_entropy_list, label='Deep Ensembles', color='r')\n",
    "sns.kdeplot(ext_sing_entropy_list, label='Single', color='g')\n",
    "plt.xlabel('External Validation Entropy Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe438d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all together\n",
    "sns.kdeplot(int_de_entropy_list, label='Int - Deep Ensembles', color='r')\n",
    "sns.kdeplot(int_sing_entropy_list, label='Int - Single', color='g')\n",
    "sns.kdeplot(ext_de_entropy_list, label='Ext - Deep Ensembles', color='r')\n",
    "sns.kdeplot(ext_sing_entropy_list, label='Ext - Single', color='g')\n",
    "plt.xlabel('Validation Entropy Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc5a94",
   "metadata": {},
   "source": [
    "### 4.3.3. Divide Groups and Check Each Group's Results\n",
    "Uncertainty cutoff 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_grouping(entropy_list, cutoff):\n",
    "    high_idx = []\n",
    "    low_idx = []\n",
    "    for idx, entropy in enumerate(entropy_list):\n",
    "        if entropy >= cutoff:\n",
    "            high_idx.append(idx)\n",
    "        else:\n",
    "            low_idx.append(idx)\n",
    "            \n",
    "    return high_idx, low_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708e454",
   "metadata": {},
   "source": [
    "#### 4.3.3.1 Internal Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-flash",
   "metadata": {},
   "source": [
    "##### 4.3.3.1.1 Deep Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c62a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_de_high, int_de_low = uc_grouping(int_de_entropy_list, cutoff=0.25)\n",
    "print(len(int_de_high), len(int_de_low))\n",
    "\n",
    "print(\"Internal Uncertainty High: %.2f%%\" % (len(int_de_high) / len(int_de_entropy_list) * 100.0))\n",
    "print(\"Internal Uncertainty Low: %.2f%%\" % (len(int_de_low) / len(int_de_entropy_list) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 위해 ensemble 결과 필요\n",
    "de_test_high = y_test[int_de_high]\n",
    "de_pred_high = de_pred[int_de_high]\n",
    "de_proba_high = de_proba[int_de_high]\n",
    "\n",
    "de_test_low = y_test[int_de_low]\n",
    "de_pred_low = de_pred[int_de_low]\n",
    "de_proba_low = de_proba[int_de_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbe0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff 0.25\n",
    "matrix(de_test_high, de_pred_high, de_proba_high)\n",
    "matrix(de_test_low, de_pred_low, de_proba_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-election",
   "metadata": {},
   "source": [
    "##### 4.3.3.1.2. Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sing_high, int_sing_low = uc_grouping(int_sing_entropy_list, cutoff=0.25)\n",
    "print(len(int_sing_high), len(int_sing_low))\n",
    "\n",
    "print(\"Internal Uncertainty High: %.2f%%\" % (len(int_sing_high) / len(int_sing_entropy_list) * 100.0))\n",
    "print(\"Internal Uncertainty Low: %.2f%%\" % (len(int_sing_low) / len(int_sing_entropy_list) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 위해 ensemble 결과 필요\n",
    "sing_test_high = y_test[int_sing_high]\n",
    "sing_pred_high = sing_pred[int_sing_high]\n",
    "sing_proba_high = sing_proba[int_sing_high]\n",
    "\n",
    "sing_test_low = y_test[int_sing_low]\n",
    "sing_pred_low = sing_pred[int_sing_low]\n",
    "sing_proba_low = sing_proba[int_sing_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff 0.25\n",
    "matrix(sing_test_high, sing_pred_high, sing_proba_high)\n",
    "matrix(sing_test_low, sing_pred_low, sing_proba_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-increase",
   "metadata": {},
   "source": [
    "#### 4.3.3.2. External Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fb49d-f456-4fac-853a-d1e52d4a1968",
   "metadata": {},
   "source": [
    "##### 4.3.3.2.1. Deep Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_de_high, ext_de_low = uc_grouping(ext_de_entropy_list, cutoff=0.25)\n",
    "print(len(ext_de_high), len(ext_de_low))\n",
    "\n",
    "print(\"External Uncertainty High: %.2f%%\" % (len(ext_de_high) / len(ext_de_entropy_list) * 100.0))\n",
    "print(\"External Uncertainty Low: %.2f%%\" % (len(ext_de_low) / len(ext_de_entropy_list) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 위해 ensemble 결과 필요\n",
    "ext_de_test_high = y_ext[ext_de_high]\n",
    "ext_de_pred_high = ext_de_pred[ext_de_high]\n",
    "ext_de_proba_high = ext_de_proba[ext_de_high]\n",
    "\n",
    "ext_de_test_low = y_ext[ext_de_low]\n",
    "ext_de_pred_low = ext_de_pred[ext_de_low]\n",
    "ext_de_proba_low = ext_de_proba[ext_de_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff 0.25\n",
    "matrix(ext_de_test_high, ext_de_pred_high, ext_de_proba_high)\n",
    "matrix(ext_de_test_low, ext_de_pred_low, ext_de_proba_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50415dc7-1062-46b8-a048-fdbe5273e9be",
   "metadata": {},
   "source": [
    "##### 4.3.3.2.2. Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_sing_high, ext_sing_low = uc_grouping(ext_sing_entropy_list, cutoff=0.25)\n",
    "print(len(ext_sing_high), len(ext_sing_low))\n",
    "\n",
    "print(\"Internal Uncertainty High: %.2f%%\" % (len(ext_sing_high) / len(ext_sing_entropy_list) * 100.0))\n",
    "print(\"Internal Uncertainty Low: %.2f%%\" % (len(ext_sing_low) / len(ext_sing_entropy_list) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 위해 ensemble 결과 필요\n",
    "ext_sing_test_high = y_ext[ext_sing_high]\n",
    "ext_sing_pred_high = ext_sing_pred[ext_sing_high]\n",
    "ext_sing_proba_high = ext_sing_proba[ext_sing_high]\n",
    "\n",
    "ext_sing_test_low = y_ext[ext_sing_low]\n",
    "ext_sing_pred_low = ext_sing_pred[ext_sing_low]\n",
    "ext_sing_proba_low = ext_sing_proba[ext_sing_low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cutoff 0.25\n",
    "matrix(ext_sing_test_high, ext_sing_pred_high, ext_sing_proba_high)\n",
    "matrix(ext_sing_test_low, ext_sing_pred_low, ext_sing_proba_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49408db",
   "metadata": {},
   "source": [
    "# 5. Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = '../'\n",
    "\n",
    "with open(f'{mask_path}/test_mask.pickle', 'rb') as f:\n",
    "    test_mask = pickle.load(f)\n",
    "    \n",
    "with open(f'{mask_path}/ext_mask.pickle', 'rb') as f:\n",
    "    ext_mask = pickle.load(f)\n",
    "\n",
    "\n",
    "print(test_mask.shape)\n",
    "print(ext_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0]) # model이 predict 한 값의 gradient 계산\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    heatmap = np.uint8(heatmap * 255) # Scale between 0-255 to visualize\n",
    "    heatmap = np.uint8(Image.fromarray(heatmap).resize((img_array.shape[1], img_array.shape[2]), Image.ANTIALIAS))/255\n",
    "    return heatmap, preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_de_heatmap_prob(img_array, de_model, last_conv_layer):\n",
    "    gradimg = np.expand_dims(img_array, axis=0) #(1, 224, 224, 3)\n",
    "    heatmap_shape = (img_array.shape[0], img_array.shape[1])\n",
    "\n",
    "    de_heatmap = np.zeros(heatmap_shape)\n",
    "    de_proba = np.zeros((1, 2)) # proba output shape\n",
    "\n",
    "    for index, m in enumerate(de_model):\n",
    "        heatmap, proba = make_gradcam_heatmap(gradimg, m, last_conv_layer)\n",
    "        de_heatmap = de_heatmap + heatmap\n",
    "        de_proba = de_proba + proba\n",
    "\n",
    "    de_heatmap = de_heatmap/5 #(224, 224) 0-1\n",
    "    de_proba = de_proba/5 #(1, 2)\n",
    "    return de_heatmap, de_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_display_gradcam_get_iou(img_dataset, last_conv_layer, label_dataset, de_model, mask_dataset, savepath, alpha=0.8):\n",
    "    total_iou = []\n",
    "    for num, img in enumerate(tqdm(img_dataset)):\n",
    "        \n",
    "        de_heatmap, de_proba = get_de_heatmap_prob(img, de_model, last_conv_layer)\n",
    "\n",
    "        predictions = \"Meta\" if de_proba.argmax(-1) == 1 else \"GBM\"\n",
    "        label = \"Meta\" if label_dataset[num].argmax(-1) == 1 else \"GBM\"\n",
    "\n",
    "        # Rescale img, heatmap to a range 0-255\n",
    "        img = Image.fromarray(img[:,:,0].astype(np.float32) * 255).convert('RGB')\n",
    "        img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "        heatmap = np.uint8(255 * de_heatmap)\n",
    "\n",
    "        # Get IoU\n",
    "        hm_label = np.where(heatmap <= 127.5, 0, heatmap)\n",
    "        hm_label = np.where(hm_label > 127.5, 1, hm_label)\n",
    "\n",
    "        mask_label = np.where(mask_dataset[num] > 0, 1, mask_dataset[num])\n",
    "        mask_label = np.ceil(mask_label)\n",
    "\n",
    "        m = MeanIoU(num_classes=2)\n",
    "        m.update_state(mask_label, hm_label)\n",
    "        iou = m.result().numpy()\n",
    "        total_iou.append(iou)\n",
    "\n",
    "        # Use jet colormap to colorize heatmap\n",
    "        jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "        # Use RGB values of the colormap\n",
    "        jet_colors = jet(np.arange(256))[:, :3]\n",
    "        jet_heatmap = jet_colors[heatmap] #range 0-1\n",
    "        jet_heatmap = np.uint8(255 * jet_heatmap)\n",
    "\n",
    "        # Superimpose the heatmap on original image\n",
    "        superimposed_img = jet_heatmap * alpha + img\n",
    "        superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "        ax[0].imshow(img, cmap='gray')\n",
    "        ax[0].set_title(\"T2\")\n",
    "        ax[1].imshow(superimposed_img)\n",
    "        ax[1].set_title(\"GradCAM - T2\")\n",
    "        ax[2].imshow(np.array(hm_label))\n",
    "        ax[2].set_title(\"Area of Pred\")\n",
    "        ax[3].imshow(np.array(mask_label))\n",
    "        ax[3].set_title(\"Area of Ground Truth\")\n",
    "        ax[4].imshow(mask_dataset[num])\n",
    "        ax[4].set_title(\"Mask\")\n",
    "\n",
    "        plt.suptitle(f'[{num}] Prediction - {predictions} / Ground Truth - {label} / IoU - {iou:.4f}',fontsize=15)\n",
    "        plt.savefig(os.path.join(savepath, f'{num}.png'))\n",
    "        #plt.show()\n",
    "    return total_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-triple",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Internal Test\n",
    "last_conv_layer = 'conv5_block16_2_conv'\n",
    "savepath = '/home/ubuntu/SJEOM/Brain-GBMmeta/GitHub/ModelingTesting/Results/220506_CAM/int_test'\n",
    "\n",
    "int_test_total_iou = save_display_gradcam_get_iou(x_test, last_conv_layer, y_test, de_model, test_mask, savepath, alpha=0.8)\n",
    "\n",
    "with open('int_test_total_iou.pickle', 'wb') as f:\n",
    "    pickle.dump(int_test_total_iou, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(int_test_total_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-latvia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# External Test\n",
    "last_conv_layer = 'conv5_block16_2_conv'\n",
    "savepath = '/home/ubuntu/SJEOM/Brain-GBMmeta/GitHub/ModelingTesting/Results/220506_CAM/ext_test'\n",
    "\n",
    "ext_test_total_iou = save_display_gradcam_get_iou(x_ext, last_conv_layer, y_ext, de_model, ext_mask, savepath, alpha=0.8)\n",
    "\n",
    "with open('ext_test_total_iou.pickle', 'wb') as f:\n",
    "    pickle.dump(ext_test_total_iou, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ext_test_total_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-background",
   "metadata": {},
   "source": [
    "# 6. Uncertainty x IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-kruger",
   "metadata": {},
   "source": [
    "## 6.1. Internal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int de uncertainty high group -> iou high/low\n",
    "int_high_un_high_iou_idx=[]\n",
    "int_high_un_low_iou_idx=[]\n",
    "\n",
    "for i in int_de_high:\n",
    "    if int_test_total_iou[i] >= 0.5:\n",
    "        int_high_un_high_iou_idx.append(i)\n",
    "    else:\n",
    "        int_high_un_low_iou_idx.append(i)\n",
    "        \n",
    "# int de uncertainty low group -> iou high/low\n",
    "int_low_un_high_iou_idx=[]\n",
    "int_low_un_low_iou_idx=[]\n",
    "\n",
    "for i in int_de_low:\n",
    "    if int_test_total_iou[i] >= 0.5:\n",
    "        int_low_un_high_iou_idx.append(i)\n",
    "    else:\n",
    "        int_low_un_low_iou_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(int_high_un_high_iou_idx), len(int_high_un_low_iou_idx))\n",
    "print(len(int_low_un_high_iou_idx), len(int_low_un_low_iou_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_test_high_un_high_iou = y_test[int_high_un_high_iou_idx]\n",
    "int_pred_high_un_high_iou = de_pred[int_high_un_high_iou_idx]\n",
    "int_proba_high_un_high_iou = de_proba[int_high_un_high_iou_idx]\n",
    "\n",
    "int_test_high_un_low_iou = y_test[int_high_un_low_iou_idx]\n",
    "int_pred_high_un_low_iou = de_pred[int_high_un_low_iou_idx]\n",
    "int_proba_high_un_low_iou = de_proba[int_high_un_low_iou_idx]\n",
    "\n",
    "int_test_low_un_high_iou = y_test[int_low_un_high_iou_idx]\n",
    "int_pred_low_un_high_iou = de_pred[int_low_un_high_iou_idx]\n",
    "int_proba_low_un_high_iou = de_proba[int_low_un_high_iou_idx]\n",
    "\n",
    "int_test_low_un_low_iou = y_test[int_low_un_low_iou_idx]\n",
    "int_pred_low_un_low_iou = de_pred[int_low_un_low_iou_idx]\n",
    "int_proba_low_un_low_iou = de_proba[int_low_un_low_iou_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(int_test_high_un_high_iou, int_pred_high_un_high_iou, int_proba_high_un_high_iou)\n",
    "matrix(int_test_high_un_low_iou, int_pred_high_un_low_iou, int_proba_high_un_low_iou)\n",
    "matrix(int_test_low_un_high_iou, int_pred_low_un_high_iou, int_proba_low_un_high_iou)\n",
    "matrix(int_test_low_un_low_iou, int_pred_low_un_low_iou, int_proba_low_un_low_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-terrorist",
   "metadata": {},
   "source": [
    "## 6.2. External Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext de uncertainty high group -> iou high/low\n",
    "ext_high_un_high_iou_idx=[]\n",
    "ext_high_un_low_iou_idx=[]\n",
    "\n",
    "for i in ext_de_high:\n",
    "    if ext_test_total_iou[i] >= 0.5:\n",
    "        ext_high_un_high_iou_idx.append(i)\n",
    "    else:\n",
    "        ext_high_un_low_iou_idx.append(i)\n",
    "        \n",
    "# ext de uncertainty low group -> iou high/low\n",
    "ext_low_un_high_iou_idx=[]\n",
    "ext_low_un_low_iou_idx=[]\n",
    "\n",
    "for i in ext_de_low:\n",
    "    if ext_test_total_iou[i] >= 0.5:\n",
    "        ext_low_un_high_iou_idx.append(i)\n",
    "    else:\n",
    "        ext_low_un_low_iou_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ext_high_un_high_iou_idx), len(ext_high_un_low_iou_idx))\n",
    "print(len(ext_low_un_high_iou_idx), len(ext_low_un_low_iou_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_test_high_un_high_iou = y_ext[ext_high_un_high_iou_idx]\n",
    "ext_pred_high_un_high_iou = ext_de_pred[ext_high_un_high_iou_idx]\n",
    "ext_proba_high_un_high_iou = ext_de_proba[ext_high_un_high_iou_idx]\n",
    "\n",
    "ext_test_high_un_low_iou = y_ext[ext_high_un_low_iou_idx]\n",
    "ext_pred_high_un_low_iou = ext_de_pred[ext_high_un_low_iou_idx]\n",
    "ext_proba_high_un_low_iou = ext_de_proba[ext_high_un_low_iou_idx]\n",
    "\n",
    "\n",
    "ext_test_low_un_high_iou = y_ext[ext_low_un_high_iou_idx]\n",
    "ext_pred_low_un_high_iou = ext_de_pred[ext_low_un_high_iou_idx]\n",
    "ext_proba_low_un_high_iou = ext_de_proba[ext_low_un_high_iou_idx]\n",
    "\n",
    "ext_test_low_un_low_iou = y_ext[ext_low_un_low_iou_idx]\n",
    "ext_pred_low_un_low_iou = ext_de_pred[ext_low_un_low_iou_idx]\n",
    "ext_proba_low_un_low_iou = ext_de_proba[ext_low_un_low_iou_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(ext_test_high_un_high_iou, ext_pred_high_un_high_iou, ext_proba_high_un_high_iou)\n",
    "matrix(ext_test_high_un_low_iou, ext_pred_high_un_low_iou, ext_proba_high_un_low_iou)\n",
    "matrix(ext_test_low_un_high_iou, ext_pred_low_un_high_iou, ext_proba_low_un_high_iou)\n",
    "matrix(ext_test_low_un_low_iou, ext_pred_low_un_low_iou, ext_proba_low_un_low_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20604f68",
   "metadata": {},
   "source": [
    "# 7. OOD Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6670d-cdbc-4849-84a9-84260e921a42",
   "metadata": {},
   "source": [
    "## 7.1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'x_ewha.pickle'), 'rb') as f:\n",
    "    x_ewha = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_dir, 'y_ewha.pickle'), 'rb') as f:\n",
    "    y_ewha = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_dir, 'x_sev.pickle'), 'rb') as f:\n",
    "    x_sev = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_dir, 'y_sev.pickle'), 'rb') as f:\n",
    "    y_sev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ewha Data Visualization\n",
    "n=0\n",
    "f, axarr = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axarr[i, j].imshow(x_ewha[n+50*i+5*j], cmap='gray') #3 channel 한 번에 띄우면 이미지가 이상해ㅜ\n",
    "        axarr[i, j].axis('off')\n",
    "        axarr[i, j].set_title(n+50*i+5*j)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d50a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severance Data Visualization\n",
    "n=0\n",
    "f, axarr = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axarr[i, j].imshow(x_sev[n+50*i+5*j], cmap='gray') #3 channel 한 번에 띄우면 이미지가 이상해ㅜ\n",
    "        axarr[i, j].axis('off')\n",
    "        axarr[i, j].set_title(n+50*i+5*j)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c41f94",
   "metadata": {},
   "source": [
    "## 7.2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ewha = keras.utils.to_categorical(y_ewha)\n",
    "y_sev = keras.utils.to_categorical(y_sev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ewha.shape, y_ewha.shape, x_sev.shape, y_sev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bcff2-3269-4ec4-bb59-2c1bb0098222",
   "metadata": {},
   "source": [
    "### 7.2.1. Ewha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61373c-fce7-491c-951a-716987585775",
   "metadata": {},
   "source": [
    "#### 7.2.1.1. Deep Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def026dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensemble\n",
    "ewha_de_sing_proba= []\n",
    "for m in de_model:\n",
    "    proba = m.predict(test_datagen.flow(x_ewha, shuffle=False, seed=42))\n",
    "    ewha_de_sing_proba.append(proba)\n",
    "    \n",
    "# Ensemble\n",
    "ewha_results = np.zeros( (y_ewha.shape[0],2) )\n",
    "for s in ewha_de_sing_proba:\n",
    "    ewha_results = ewha_results + s\n",
    "\n",
    "ewha_de_proba = ewha_results/5\n",
    "ewha_de_pred = (ewha_de_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_ewha, ewha_de_pred, ewha_de_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf695011-da8d-46de-8ecd-4654d423918c",
   "metadata": {},
   "source": [
    "#### 7.2.1.2. Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "ewha_sing_proba = sing_model.predict(test_datagen.flow(x_ewha, shuffle=False, seed=42))\n",
    "ewha_sing_pred = (ewha_sing_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ewha.shape, ewha_sing_pred.shape, ewha_sing_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_ewha, ewha_sing_pred, ewha_sing_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0fdd7-614c-4991-b6b0-3cacff4afac8",
   "metadata": {},
   "source": [
    "#### 7.2.1.3. Uncertainty (Entropy) Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27276d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ewha_de_entropy = []\n",
    "for i in range(len(ewha_de_proba)):\n",
    "    pq = ewha_de_proba[i] * y_ewha[i]\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    ewha_de_entropy.append(entropy)\n",
    "    \n",
    "ewha_sing_entropy = []\n",
    "for i in range(len(ewha_sing_proba)):\n",
    "    pq = ewha_sing_proba[i] * y_ewha[i]\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p+ 0.00000000001) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    ewha_sing_entropy.append(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebd705-118c-4ed1-be3e-076ca2e7d1aa",
   "metadata": {},
   "source": [
    "#### 7.2.1.4. Divide Groups with Entropy Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensemble\n",
    "ewha_de_high, ewha_de_low = uc_grouping(ewha_de_entropy, cutoff=0.25)\n",
    "print(len(ewha_de_high), len(ewha_de_low))\n",
    "\n",
    "print(\"Ewha Uncertainty High: %.2f%%\" % (len(ewha_de_high) / len(ewha_de_entropy) * 100.0))\n",
    "print(\"Ewha Uncertainty Low: %.2f%%\" % (len(ewha_de_low) / len(ewha_de_entropy) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "ewha_sing_high, ewha_sing_low = uc_grouping(ewha_sing_entropy, cutoff=0.25)\n",
    "print(len(ewha_sing_high), len(ewha_sing_low))\n",
    "\n",
    "print(\"Ewha Uncertainty High: %.2f%%\" % (len(ewha_sing_high) / len(ewha_sing_entropy) * 100.0))\n",
    "print(\"Ewha Uncertainty Low: %.2f%%\" % (len(ewha_sing_low) / len(ewha_sing_entropy) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution\n",
    "sns.kdeplot(np.array(ewha_de_entropy), label='ensemble', color='r')\n",
    "sns.kdeplot(np.array(ewha_sing_entropy), label='single', color='g')\n",
    "plt.xlabel('OOD; Ewha Entropy Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d1ecb-111b-4e1c-86f7-644b81a0324b",
   "metadata": {},
   "source": [
    "### 7.2.2. Severance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b4142-577c-4025-8ecc-98e2a86c0dd2",
   "metadata": {},
   "source": [
    "#### 7.2.2.1. Deep Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_de_sing_proba= []\n",
    "for m in de_model:\n",
    "    proba = m.predict(test_datagen.flow(x_sev, shuffle=False, seed=42))\n",
    "    sev_de_sing_proba.append(proba)\n",
    "    \n",
    "# Ensemble\n",
    "sev_results = np.zeros( (y_sev.shape[0],2) )\n",
    "for s in sev_de_sing_proba:\n",
    "    sev_results = sev_results + s\n",
    "\n",
    "sev_de_proba = sev_results/5\n",
    "sev_de_pred = (sev_de_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sev.shape, sev_de_proba.shape, sev_de_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_sev, sev_de_pred, sev_de_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb7d2a-0a11-4593-b674-2998a1db6126",
   "metadata": {},
   "source": [
    "#### 7.2.2.2. Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_sing_proba = sing_model.predict(test_datagen.flow(x_sev, shuffle=False, seed=42))\n",
    "sev_sing_pred = (sev_sing_proba > 0.5).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sev.shape, sev_sing_proba.shape, sev_sing_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix(y_sev, sev_sing_pred, sev_sing_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8f25e-3651-439b-8128-594e80b242b7",
   "metadata": {},
   "source": [
    "#### 7.2.2.3. Uncertainty (Entropy) Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_de_entropy = []\n",
    "for i in range(len(sev_de_proba)):\n",
    "    pq = sev_de_proba[i] * y_sev[i]\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    sev_de_entropy.append(entropy)\n",
    "    \n",
    "sev_sing_entropy = []\n",
    "for i in range(len(sev_sing_proba)):\n",
    "    pq = sev_sing_proba[i] * y_sev[i]\n",
    "    index = np.argmax(pq)\n",
    "    p = pq[index]\n",
    "    entropy = -p*math.log(p+ 0.00000000001) - (1-p)*math.log(1-p+ 0.00000000001)\n",
    "    sev_sing_entropy.append(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bde4b4-3ffa-46ce-b3e1-c4da3f296fda",
   "metadata": {},
   "source": [
    "#### 7.2.2.4. Divide Groups with Entropy Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensembles\n",
    "sev_de_high, sev_de_low = uc_grouping(sev_de_entropy, cutoff=0.25)\n",
    "print(len(sev_de_high), len(sev_de_low))\n",
    "\n",
    "print(\"Severance Uncertainty High: %.2f%%\" % (len(sev_de_high) / len(sev_de_entropy) * 100.0))\n",
    "print(\"Severance Uncertainty Low: %.2f%%\" % (len(sev_de_low) / len(sev_de_entropy) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "sev_sing_high, sev_sing_low = uc_grouping(sev_sing_entropy, cutoff=0.25)\n",
    "print(len(sev_sing_high), len(sev_sing_low))\n",
    "\n",
    "print(\"Severance Uncertainty High: %.2f%%\" % (len(sev_sing_high) / len(sev_sing_entropy) * 100.0))\n",
    "print(\"Severance Uncertainty Low: %.2f%%\" % (len(sev_sing_low) / len(sev_sing_entropy) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution\n",
    "sns.kdeplot(np.array(sev_de_entropy), label='ensemble', color='r')\n",
    "sns.kdeplot(np.array(sev_sing_entropy), label='single', color='g')\n",
    "plt.xlabel('OOD; Severance Entropy Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[Brain-GBMmeta] Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
